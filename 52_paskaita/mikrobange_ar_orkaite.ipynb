{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# nurodom kelia i katalogus\n",
    "data_dir = 'pigu'\n",
    "mikrobange_dir = os.path.join(data_dir, 'mikrobanges')\n",
    "orkaite_dir = os.path.join(data_dir, 'orkaites')\n",
    "\n",
    "# aprasyti, kokia dalis tenka mokymui, testams ir validacijai\n",
    "splits = (0.7, 0.15, 0.15)\n",
    "\n",
    "def split_data(directory:str, splits:tuple) -> None:\n",
    "    \"\"\"\n",
    "    Funkcija yra skirta suskaidyti pateiktam kataloge esancias nuotraukas i tris naujus katalogus, pagal \n",
    "    pateiktus isskaidymo dydzius\n",
    "\n",
    "    Parametrai:\n",
    "    directory - nuoroda iki failo, kuri norite skaidyti\n",
    "    splits - tuple, su nurodytais kiekiais mokymui, testavimui ir validacijai\n",
    "    \"\"\"\n",
    "    images = os.listdir(directory) # gauname visas nuotraukas\n",
    "    random.shuffle(images) # ismaisome nuotraukas, siekiant skirtingu paledimu metu tureti skirtingus duomenis\n",
    "    # norime suzinoti kiekius, kiek nuotrauku reikes mokymams\n",
    "    train_size = int(len(images) * splits[0])\n",
    "    validation_size = int(len(images) * splits[1])\n",
    "    test_size = int(len(images) * splits[2])\n",
    "    \n",
    "    # katalogu sukurimas\n",
    "    train_dir = os.path.join(directory, 'train')\n",
    "    validation_dir = os.path.join(directory, 'validation')\n",
    "    test_dir = os.path.join(directory, 'test')\n",
    "\n",
    "    # os.removedirs(train_dir)\n",
    "    # os.removedirs(validation_dir)\n",
    "    # os.removedirs(test_dir)\n",
    "\n",
    "    # TODO: sunaikinti katalogus, pries tai sunaikinant turini juose, tai leis isvengti is a directory klaidos\n",
    "\n",
    "    # katalogu sukurimas, pagal pateiktas nuorodas\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        if i < train_size:\n",
    "            shutil.copy(os.path.join(directory, image), os.path.join(train_dir, image))\n",
    "        elif i < train_size + validation_size:\n",
    "            shutil.copy(os.path.join(directory, image), os.path.join(validation_dir, image))\n",
    "        else: \n",
    "            shutil.copy(os.path.join(directory, image), os.path.join(test_dir, image))\n",
    "\n",
    "\n",
    "\n",
    "split_data(mikrobange_dir, splits)\n",
    "split_data(orkaite_dir, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasiekti nuotraukas\n",
    "# patikrinti ar nuotrauka validi\n",
    "# suvienodinti nuotraukas\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "data_dir = 'pigu'\n",
    "mikrobange_dir = os.path.join(data_dir, 'mikrobanges')\n",
    "orkaite_dir = os.path.join(data_dir, 'orkaites')\n",
    "\n",
    "# skirta patikrinti ar atidarant negausime klaidos, kaip argumenta pateikiame kelia iki nuotraukos\n",
    "def is_valid_image(file_path):\n",
    "    try:\n",
    "        with Image.open(file_path) as img:\n",
    "            # patikrina ar nuotrauka galima atidaryti\n",
    "            img.verify()\n",
    "        return True\n",
    "    except (IOError, SyntaxError):\n",
    "        return False\n",
    "\n",
    "def get_valid_image_files(directory):\n",
    "    \"\"\"\n",
    "    Skirta filtruoti direktorijoje esancius failus, patikrinti ar jie yra validus. Rezultatas failu pavadinimu sarasas, su validziu failu pavadinimasis\n",
    "    \"\"\"\n",
    "    valid_files = []\n",
    "    # naudojame _, nes neketiname naudoti katalogu(train, test, validation)\n",
    "    for root, _, files in os.walk(directory):\n",
    "        if root != directory:\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if is_valid_image(file_path):\n",
    "                    valid_files.append(file_path)\n",
    "    return valid_files\n",
    "\n",
    "\n",
    "mikrobange_photos = get_valid_image_files(mikrobange_dir)\n",
    "orkaite_photos = get_valid_image_files(orkaite_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2421 validated image filenames.\n",
      "Found 2261 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd \n",
    "\n",
    "# naudojame siekiant sumazinti pixeliu vertes is intervalo 0-255 i intervala 0-1\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# kuriame df, nes tai yra budas perteikti informacija generatoriui\n",
    "mikrobange_df = pd.DataFrame({'filename': mikrobange_photos})\n",
    "orkaite_df = pd.DataFrame({'filename': orkaite_photos})\n",
    "\n",
    "mikrobange_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = mikrobange_df, # nurodom kur yra musu nuotrauku sarasas\n",
    "    x_col = 'filename', # nurodom kuris stulpelis yra failo kelias musu df\n",
    "    target_size = (150, 150), # nurodom nuotrauku dydzius\n",
    "    batch_size = 20, # nurodom kiek nuotrauku idesime kiekvieno iteracijos metu\n",
    "    class_mode = None) # mes turime tik du galimus outputus, todel class mode nustatome None\n",
    "\n",
    "orkaite_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = orkaite_df, # nurodom kur yra musu nuotrauku sarasas\n",
    "    x_col = 'filename', # nurodom kuris stulpelis yra failo kelias musu df\n",
    "    target_size = (150, 150), # nurodom nuotrauku dydzius\n",
    "    batch_size = 20, # nurodom kiek nuotrauku idesime kiekvieno iteracijos metu\n",
    "    class_mode = None) # mes turime tik du galimus outputus, todel class mode nustatome None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# kuriame sia klase tam, kad turetume galimybe training matricas pateikti gabalais (batches)\n",
    "class CombinedGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, *generators):\n",
    "        self.generators = generators\n",
    "        self._num_batches = sum(len(gen) for gen in generators)\n",
    "        self.current_generator = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._num_batches\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        for gen in self.generators:\n",
    "            if idx < len(gen):\n",
    "                batch = gen[idx]\n",
    "                # generuojame labels, atsizvelgdami i tai kiek nariu turime savo batche\n",
    "                labels = np.array([0] * batch.shape[0]) if gen == mikrobange_generator else np.array([1] * batch.shape[0])\n",
    "                return batch, labels\n",
    "            idx -= len(gen)\n",
    "\n",
    "combined_generator = CombinedGenerator(mikrobange_generator, orkaite_generator)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ostap\\Desktop\\Mokymai\\Duomenu_mokslas\\mokymai\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation = 'relu', input_shape=(150,150,3)), #nurodom filtrus ir ju dydziu dimensija\n",
    "    layers.MaxPooling2D(2,2), #sumazinam dimensijas, islaikant svarbiausias savybes\n",
    "    layers.Conv2D(64, (3,3), activation = 'relu'), #antras konvoliucinis sloksnis \n",
    "    layers.MaxPooling2D(2,2), #sumazinam dimensijas, islaikant svarbiausias savybes\n",
    "    layers.Conv2D(128, (3,3), activation = 'relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Flatten(), #plokstinam duomenis, paversdami is 3D i 1D\n",
    "    layers.Dense(512, activation= 'relu'),\n",
    "    layers.Dense(1, activation='sigmoid') #isejimo sluoksnis su vienu neuronu, grazins tikimybe nuo 0 iki 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy', #taikome, nes musu klasifikojami duomenys bus 0 arba 1\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ostap\\Desktop\\Mokymai\\Duomenu_mokslas\\mokymai\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 312ms/step - accuracy: 0.5758 - loss: 1.1516\n",
      "Epoch 2/10\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1008.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 307ms/step - accuracy: 0.8113 - loss: 0.4083\n",
      "Epoch 4/10\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 293ms/step - accuracy: 0.8906 - loss: 0.2561\n",
      "Epoch 6/10\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 286ms/step - accuracy: 0.9221 - loss: 0.1830\n",
      "Epoch 8/10\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 286ms/step - accuracy: 0.9453 - loss: 0.1240\n",
      "Epoch 10/10\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    combined_generator, #nurodom generatoriu, is kurio duomenis imsime dalimis\n",
    "    steps_per_epoch = len(combined_generator), #pasiims batchu kieki\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((150,150))\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Mikrobange\n",
      "--------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Mikrobange\n",
      "--------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Orkaite\n",
      "--------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Orkaite\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "def predict_image(image_path):\n",
    "    img_array = preprocess_image(image_path)\n",
    "    prediction = model.predict(img_array)\n",
    "    return \"Orkaite\" if prediction[0][0] > 0.5 else 'Mikrobange'\n",
    "\n",
    "print(predict_image('mikro.jpg'))\n",
    "print('--------------')\n",
    "print(predict_image('mikro2.jpg'))\n",
    "print('--------------')\n",
    "print(predict_image('orkaite.jpeg'))\n",
    "print('--------------')\n",
    "print(predict_image('orkaite2.jpg'))\n",
    "print('--------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mokymai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
